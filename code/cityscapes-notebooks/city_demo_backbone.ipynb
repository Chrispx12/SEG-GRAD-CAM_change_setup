{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate Seg-Grad-CAM on a U-Net with a backbone from 'segmentation_models' trained on Cityscapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import segmentation_models as sm\n",
    "\n",
    "from seggradcam.dataloaders import Cityscapes\n",
    "#from metrics import iou_coef, dice_coef, dice_loss\n",
    "from seggradcam.unet import csbd_unet, manual_unet, TrainUnet\n",
    "from seggradcam.training_write import TrainingParameters, TrainingResults\n",
    "from seggradcam.training_plots import plot_predict_and_gt, plot_loss, plot_metric\n",
    "from seggradcam.seggradcam import SegGradCAM, SuperRoI, ClassRoI, PixelRoI, BiasRoI\n",
    "from seggradcam.visualize_sgc import SegGradCAMplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "trainparam = TrainingParameters(\n",
    "    epochs = 50,\n",
    "    scale = 4,\n",
    "    batch_size = BATCH_SIZE, \n",
    "    n_train= 2975, #2975 max\n",
    "    n_val = 500 # 500 max \n",
    "    ,steps_per_epoch = int(3000/BATCH_SIZE)\n",
    "    ,validation_steps = int(500/BATCH_SIZE)   \n",
    "    )\n",
    "trainparam.saveToJson()\n",
    "\n",
    "trainset = Cityscapes(n = trainparam.n_train, shuffle = True, scale = trainparam.scale, prefix = \"train\",normalize=True)\n",
    "valset = Cityscapes(n = trainparam.n_val, shuffle = False, scale = trainparam.scale, prefix = \"val\",normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 A. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path:  C:\\Users\\vinograd\\Documents\\cityscapes\\leftImg8bit_trainvaltest\\leftImg8bit\\train\n",
      "Length of the set: 2975 .  2975  will be loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2975/2975 [09:14<00:00,  6.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2975/2975 [01:43<00:00, 28.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path:  C:\\Users\\vinograd\\Documents\\cityscapes\\leftImg8bit_trainvaltest\\leftImg8bit\\val\n",
      "Length of the set: 500 .  500  will be loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [02:28<00:00,  5.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:18<00:00, 27.35it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\vinograd\\\\Documents\\\\cityscapes\\\\leftImg8bit_trainvaltest\\\\leftImg8bit\"\n",
    "#path = \"../../inputs/cityscapes/leftImg8bit_trainvaltest/leftImg8bit\"\n",
    "trainset.get_and_save_npz(path)\n",
    "valset.get_and_save_npz(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 B. Load prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.load_npz()\n",
    "valset.load_npz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. U-Net set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "BACKBONE = 'vgg16'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "n_classes = trainparam.n_classes\n",
    "activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "#create model\n",
    "model = sm.Unet(BACKBONE, classes=n_classes, activation=activation, encoder_weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 A. Train a U-Net yourself "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: '../../output/sm_vgg16_09_30_16_14'\n"
     ]
    }
   ],
   "source": [
    "LR = 0.00045\n",
    "EPOCHS = 2 # We recommend training for at least 150\n",
    "\"\"\"\n",
    "# check shapes for errors\n",
    "assert train_dataloader[0][0].shape == (BATCH_SIZE, 320, 320, 3)\n",
    "assert train_dataloader[0][1].shape == (BATCH_SIZE, 320, 320, n_classes)\n",
    "\"\"\"\n",
    "# define callbacks for learning rate scheduling and best checkpoints saving\n",
    "import keras\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now()\n",
    "mdir = '../../output/sm_'+BACKBONE+'_'+str(timestamp.strftime(\"%m_%d_%H_%M\"))\n",
    "try:\n",
    "    os.mkdir(mdir)\n",
    "except OSError as error: \n",
    "    print(error) \n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(mdir+'best_model.h5', save_weights_only=True, save_best_only=True, mode='min'),\n",
    "    keras.callbacks.ReduceLROnPlateau(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vinograd\\Anaconda3_12\\envs\\feb20tfclone\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optim = keras.optimizers.Adam(LR)\n",
    "\n",
    "dice_loss = sm.losses.DiceLoss()\n",
    "focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "\n",
    "model.compile(optim, total_loss, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vinograd\\Anaconda3_12\\envs\\feb20tfclone\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      " 85/750 [==>...........................] - ETA: 4:49:14 - loss: 0.9501 - iou_score: 0.0131 - f1-score: 0.024 - ETA: 3:42:37 - loss: 0.9367 - iou_score: 0.0190 - f1-score: 0.033 - ETA: 3:23:34 - loss: 0.9280 - iou_score: 0.0277 - f1-score: 0.045 - ETA: 3:11:27 - loss: 0.9200 - iou_score: 0.0297 - f1-score: 0.048 - ETA: 3:04:52 - loss: 0.9111 - iou_score: 0.0323 - f1-score: 0.052 - ETA: 2:51:58 - loss: 0.9051 - iou_score: 0.0328 - f1-score: 0.053 - ETA: 2:41:27 - loss: 0.8976 - iou_score: 0.0370 - f1-score: 0.058 - ETA: 2:33:23 - loss: 0.8932 - iou_score: 0.0372 - f1-score: 0.058 - ETA: 2:27:16 - loss: 0.8874 - iou_score: 0.0395 - f1-score: 0.061 - ETA: 2:22:44 - loss: 0.8813 - iou_score: 0.0420 - f1-score: 0.063 - ETA: 2:19:30 - loss: 0.8760 - iou_score: 0.0441 - f1-score: 0.065 - ETA: 2:16:32 - loss: 0.8706 - iou_score: 0.0453 - f1-score: 0.066 - ETA: 2:13:45 - loss: 0.8650 - iou_score: 0.0470 - f1-score: 0.068 - ETA: 2:11:16 - loss: 0.8613 - iou_score: 0.0482 - f1-score: 0.069 - ETA: 2:08:43 - loss: 0.8580 - iou_score: 0.0487 - f1-score: 0.070 - ETA: 2:06:25 - loss: 0.8548 - iou_score: 0.0496 - f1-score: 0.070 - ETA: 2:04:14 - loss: 0.8513 - iou_score: 0.0507 - f1-score: 0.072 - ETA: 2:02:24 - loss: 0.8484 - iou_score: 0.0520 - f1-score: 0.074 - ETA: 2:01:04 - loss: 0.8455 - iou_score: 0.0538 - f1-score: 0.076 - ETA: 1:59:28 - loss: 0.8421 - iou_score: 0.0578 - f1-score: 0.082 - ETA: 1:58:10 - loss: 0.8385 - iou_score: 0.0632 - f1-score: 0.088 - ETA: 1:57:10 - loss: 0.8353 - iou_score: 0.0684 - f1-score: 0.095 - ETA: 1:55:58 - loss: 0.8323 - iou_score: 0.0730 - f1-score: 0.101 - ETA: 1:54:47 - loss: 0.8289 - iou_score: 0.0776 - f1-score: 0.106 - ETA: 1:53:45 - loss: 0.8254 - iou_score: 0.0821 - f1-score: 0.112 - ETA: 1:52:50 - loss: 0.8226 - iou_score: 0.0854 - f1-score: 0.116 - ETA: 1:51:50 - loss: 0.8200 - iou_score: 0.0888 - f1-score: 0.121 - ETA: 1:50:57 - loss: 0.8180 - iou_score: 0.0917 - f1-score: 0.125 - ETA: 1:50:11 - loss: 0.8160 - iou_score: 0.0951 - f1-score: 0.129 - ETA: 1:49:25 - loss: 0.8134 - iou_score: 0.0987 - f1-score: 0.134 - ETA: 1:48:38 - loss: 0.8104 - iou_score: 0.1028 - f1-score: 0.139 - ETA: 1:47:52 - loss: 0.8078 - iou_score: 0.1063 - f1-score: 0.144 - ETA: 1:47:07 - loss: 0.8050 - iou_score: 0.1100 - f1-score: 0.149 - ETA: 1:46:26 - loss: 0.8021 - iou_score: 0.1138 - f1-score: 0.154 - ETA: 1:45:50 - loss: 0.8008 - iou_score: 0.1158 - f1-score: 0.156 - ETA: 1:45:13 - loss: 0.7984 - iou_score: 0.1187 - f1-score: 0.160 - ETA: 1:44:42 - loss: 0.7969 - iou_score: 0.1207 - f1-score: 0.163 - ETA: 1:44:11 - loss: 0.7963 - iou_score: 0.1220 - f1-score: 0.165 - ETA: 1:43:43 - loss: 0.7947 - iou_score: 0.1243 - f1-score: 0.168 - ETA: 1:43:13 - loss: 0.7941 - iou_score: 0.1257 - f1-score: 0.170 - ETA: 1:42:49 - loss: 0.7917 - iou_score: 0.1286 - f1-score: 0.174 - ETA: 1:42:23 - loss: 0.7903 - iou_score: 0.1305 - f1-score: 0.177 - ETA: 1:41:57 - loss: 0.7877 - iou_score: 0.1337 - f1-score: 0.181 - ETA: 1:41:36 - loss: 0.7859 - iou_score: 0.1363 - f1-score: 0.184 - ETA: 1:41:15 - loss: 0.7844 - iou_score: 0.1383 - f1-score: 0.187 - ETA: 1:40:53 - loss: 0.7833 - iou_score: 0.1399 - f1-score: 0.190 - ETA: 1:40:31 - loss: 0.7818 - iou_score: 0.1421 - f1-score: 0.193 - ETA: 1:40:06 - loss: 0.7799 - iou_score: 0.1445 - f1-score: 0.196 - ETA: 1:39:45 - loss: 0.7778 - iou_score: 0.1471 - f1-score: 0.199 - ETA: 1:39:25 - loss: 0.7766 - iou_score: 0.1490 - f1-score: 0.202 - ETA: 1:39:08 - loss: 0.7748 - iou_score: 0.1509 - f1-score: 0.204 - ETA: 1:38:51 - loss: 0.7738 - iou_score: 0.1525 - f1-score: 0.207 - ETA: 1:38:33 - loss: 0.7721 - iou_score: 0.1544 - f1-score: 0.209 - ETA: 1:38:12 - loss: 0.7706 - iou_score: 0.1561 - f1-score: 0.211 - ETA: 1:37:53 - loss: 0.7689 - iou_score: 0.1581 - f1-score: 0.214 - ETA: 1:37:34 - loss: 0.7674 - iou_score: 0.1600 - f1-score: 0.216 - ETA: 1:37:15 - loss: 0.7659 - iou_score: 0.1618 - f1-score: 0.218 - ETA: 1:37:02 - loss: 0.7646 - iou_score: 0.1631 - f1-score: 0.220 - ETA: 1:36:48 - loss: 0.7632 - iou_score: 0.1648 - f1-score: 0.222 - ETA: 1:36:34 - loss: 0.7619 - iou_score: 0.1662 - f1-score: 0.224 - ETA: 1:36:21 - loss: 0.7608 - iou_score: 0.1674 - f1-score: 0.226 - ETA: 1:36:14 - loss: 0.7607 - iou_score: 0.1676 - f1-score: 0.227 - ETA: 1:36:00 - loss: 0.7593 - iou_score: 0.1691 - f1-score: 0.229 - ETA: 1:35:44 - loss: 0.7583 - iou_score: 0.1702 - f1-score: 0.230 - ETA: 1:35:36 - loss: 0.7572 - iou_score: 0.1713 - f1-score: 0.232 - ETA: 1:35:29 - loss: 0.7559 - iou_score: 0.1726 - f1-score: 0.233 - ETA: 1:35:15 - loss: 0.7553 - iou_score: 0.1737 - f1-score: 0.235 - ETA: 1:35:03 - loss: 0.7547 - iou_score: 0.1746 - f1-score: 0.236 - ETA: 1:34:51 - loss: 0.7535 - iou_score: 0.1761 - f1-score: 0.238 - ETA: 1:34:37 - loss: 0.7521 - iou_score: 0.1776 - f1-score: 0.240 - ETA: 1:34:24 - loss: 0.7511 - iou_score: 0.1786 - f1-score: 0.241 - ETA: 1:34:11 - loss: 0.7496 - iou_score: 0.1805 - f1-score: 0.244 - ETA: 1:33:59 - loss: 0.7487 - iou_score: 0.1814 - f1-score: 0.245 - ETA: 1:33:48 - loss: 0.7477 - iou_score: 0.1824 - f1-score: 0.246 - ETA: 1:33:40 - loss: 0.7465 - iou_score: 0.1837 - f1-score: 0.248 - ETA: 1:33:31 - loss: 0.7458 - iou_score: 0.1846 - f1-score: 0.249 - ETA: 1:33:21 - loss: 0.7442 - iou_score: 0.1862 - f1-score: 0.251 - ETA: 1:33:11 - loss: 0.7431 - iou_score: 0.1873 - f1-score: 0.252 - ETA: 1:32:58 - loss: 0.7420 - iou_score: 0.1885 - f1-score: 0.254 - ETA: 1:32:44 - loss: 0.7411 - iou_score: 0.1894 - f1-score: 0.255 - ETA: 1:32:30 - loss: 0.7399 - iou_score: 0.1907 - f1-score: 0.257 - ETA: 1:32:17 - loss: 0.7394 - iou_score: 0.1913 - f1-score: 0.257 - ETA: 1:32:03 - loss: 0.7385 - iou_score: 0.1922 - f1-score: 0.259 - ETA: 1:31:52 - loss: 0.7373 - iou_score: 0.1935 - f1-score: 0.260 - ETA: 1:31:39 - loss: 0.7358 - iou_score: 0.1950 - f1-score: 0.2623"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "            rescale=1. / 255,\n",
    "            # brightness_range = (.8,1.2),\n",
    "            horizontal_flip=True)\n",
    "train_input = datagen.flow(trainset.X, trainset.Y, batch_size=BATCH_SIZE)\n",
    "val_input = datagen.flow(valset.X, valset.Y, batch_size=BATCH_SIZE) \n",
    "\n",
    "history = model.fit_generator(train_input,\n",
    "                                      validation_data=val_input,\n",
    "                                      epochs=EPOCHS,\n",
    "                                      steps_per_epoch=trainparam.steps_per_epoch,\n",
    "                                      validation_steps=trainparam.validation_steps,\n",
    "                                      callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation iou_score values\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['iou_score'])\n",
    "plt.plot(history.history['val_iou_score'])\n",
    "plt.plot(history.history['f1-score'])\n",
    "plt.plot(history.history['val_f1-score'])\n",
    "plt.title('Model IoU & F1 scores')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train IoU', 'Val IoU','Train F1','Val F1'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.savefig(mdir+'iou_f1_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 B. Load a U-Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdir = '../../output/unet_backbone_16_07/'\n",
    "model.load_weights(mdir+'best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Take a look at predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predict_and_gt(model, valset.X, valset.Y, range(0,10,1), mdir, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Seg-Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Set up layers for propagation of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_from_layer = model.layers[-1].name\n",
    "prop_to_layer = 'center_block1_relu'\n",
    "# We recommend to try also:\n",
    "#prop_to_layer = 'center_block2_relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Choose an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = 7 # car\n",
    "imid = 3\n",
    "image =  valset.X[imid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 A. Seg-Grad-CAM for a pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SegGradCAM object\n",
    "roi=PixelRoI(500,600,image)\n",
    "pixsgc = SegGradCAM(trainunet.model, image, cls,  prop_to_layer,prop_from_layer, roi=roi,\n",
    "                 normalize=True, abs_w=False, posit_w=False)\n",
    "# compute SegGradCAM\n",
    "pixsgc.SGC()\n",
    "# create an object with plotting functionality\n",
    "plotter = SegGradCAMplot(pixsgc,trainunet, gt = trainset.Y[imid])\n",
    "# plot explanations on 1 picture\n",
    "plotter.explainPixel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 4 images: original, ground truth, predicted mask, seg-grad-cam explanations for a selected single pixel\n",
    "plotter.pixelGtPrediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 B. Seg-Grad-CAM for the class (all pixels that were predicted as class 'cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsroi = ClassRoI(model=model,image=image,cls=cls)\n",
    "newsgc = SegGradCAM(model, image, cls, prop_to_layer,  prop_from_layer, roi=clsroi,\n",
    "                 normalize=True, abs_w=False, posit_w=False)\n",
    "newsgc.SGC()\n",
    "\n",
    "# create an object with plotting functionality\n",
    "plotter = SegGradCAMplot(newsgc,model=model,n_classes=n_classes,outfolder=mdir, gt = valset.Y[imid])\n",
    "# plot explanations on 1 picture\n",
    "plotter.explainClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.classGtPrediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 C. Seg-Grad-CAM for a region of interest\n",
    "### 3.3.C.1 for the largest set of connected pixels predicted as 'cls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsroi.largestComponent()\n",
    "newsgc = SegGradCAM(model, image, cls, prop_to_layer,  prop_from_layer, roi=clsroi,\n",
    "                 normalize=True, abs_w=False, posit_w=False)\n",
    "newsgc.SGC()\n",
    "\n",
    "plotter = SegGradCAMplot(newsgc,model=model,n_classes=n_classes,outfolder=mdir, gt = valset.Y[imid])\n",
    "plotter.explainRoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.roiGtPrediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.C.2 for the smallest set of connected pixels predicted as 'cls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsroi.smallestComponent()\n",
    "newsgc = SegGradCAM(model, image, cls, prop_to_layer,  prop_from_layer, roi=clsroi,\n",
    "                 normalize=True, abs_w=False, posit_w=False)\n",
    "newsgc.SGC()\n",
    "\n",
    "plotter = SegGradCAMplot(newsgc,model=model,n_classes=n_classes,outfolder=mdir, gt = valset.Y[imid])\n",
    "plotter.explainRoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.roiGtPrediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.C.3 It is also possible to define an arbitrary RoI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
